# LLM Server â€“ FastAPI + vLLM on Kubernetes

Ce projet expose une API REST (`/api/chat`) pour interagir avec un modÃ¨le de langage via [vLLM](https://github.com/vllm-project/vllm), avec support GPU, batching, sticky session pour KV cache optimisation et mise Ã  lâ€™Ã©chelle Kubernetes.

## ğŸš€ FonctionnalitÃ©s

* API REST via FastAPI
* ModÃ¨le LLM servi via vLLM
* Sticky session via header pour optimisation KV cache
* Configuration dynamique via Helm
* Logs formattÃ©s JSON sur stdout prÃªts pour collecte
* Monitoring Prometheus (/metrics exposÃ©s)
* Test de charge via `wrk`

---

## ğŸ“¦ DÃ©ploiement

### 1. PrÃ©requis

* Kubernetes (testÃ© avec `minikube`)
* Helm â‰¥ 3.x
* GPU (ou `useGPU: false` pour CPU-only). âš ï¸ vLLM trÃ¨s instable sur CPU, nombreuses erreurs relevÃ©es et non support de WSL2 + CPU
* NGINX ingress controller ou Traefik configurable (non dÃ©ployÃ©)

### 2. Images Docker

Deux options sont possibles :

#### a. Utiliser les images prÃ©-construites du dÃ©pÃ´t public DockerHub (TinyLlama-1.1B)

```yaml
fastapi.image: docktet/fastapi:v1.1.9
vllm.image.repository: docktet/vllm-tinyllama # ~4 Gb
vllm.image.tag: latest
```

#### b. Construire les images localement (avec choix de modÃ¨le)

```bash
docker build -t <nom_de_repo>/<nom_image>:<tag> -f Dockerfile.fastapi .
docker build -t <nom_de_repo>/<nom_image>:<tag> -f Dockerfile.vllm . # NÃ©cessite de placer un modÃ¨le supportÃ© vLLM dans /models
```

### 3. DÃ©ploiement avec Helm

```bash
helm upgrade --install llm-app ./helm-chart \
  --namespace llm-app --create-namespace
```

#### Exemple de values.yaml :

```yaml
namespace: llm-app

fastapi:
  name: fastapi
  image: docktet/fastapi:v1.1.9 # Image publique avec modÃ¨le prÃ©chargÃ© 
  serviceAccount:
    name: fastapi
  containerPort: 3000
  replicaCount: 2
  modelDisplayName: "TinyLlama-1.1B" # Nom de modÃ¨le affichÃ© dans les rÃ©ponses
  resources:
    requests:
      cpu: "250m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

vllm:
  name: vllm
  image:
    repository: docktet/vllm-tinyllama # Image publique avec modÃ¨le prÃ©chargÃ© 
    tag: latest
  port: 8000
  replicaCount: 1
  useGPU: true # Si True, GPU utilisÃ© (nÃ©cessite support de Nvidia device plugin)
  resources: # Uniquement si useGPU = false
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

ingress:
  enabled: true # DÃ©ploiement ingress pour exposition si true
  type: standard # "traefik" ou "standard"
  className: nginx # Ingress class name (e.g., nginx, traefik)
  host: fastapi.localhost
```

### 4. Nettoyage

```bash
helm uninstall llm-app -n llm-app
kubectl delete namespace llm-app
```

---

## ğŸŒ Endpoints exposÃ©s

| Endpoint    | Description          |
| ----------- | -------------------- |
| `/api/chat` | Interroger le modÃ¨le |
| `/metrics`  | Export Prometheus    |
| `/ready`    | Readiness probe      |
| `/healthz`  | Liveness probe       |

---

## ğŸ§¹ Architecture â€“ composants principaux

Lâ€™application est composÃ©e de deux services principaux dÃ©ployÃ©s dans KubernetesÂ :

* **FastAPI Gateway**

  * Expose `/api/chat`, `/metrics`, `/healthz`, `/ready`
  * RÃ©partit les requÃªtes vers vLLM via `X-Session-ID`
  * IntÃ¨gre un client Kubernetes pour dÃ©couverte dynamique des pods vLLM

* **vLLM Backend**

  * Expose les endpoints OpenAI compatibles (`/v1/chat/completions`, etc.)
  * Effectue la gÃ©nÃ©ration de texte avec batching et cache KV GPU
  * Peut Ãªtre configurÃ© avec ou sans GPU (âš ï¸ CPU instable)

ğŸ” Ces services communiquent via le rÃ©seau interne du cluster (service Kubernetes).

---

## ğŸ” Sticky sessions (optimisation cache)

Lâ€™API `/api/chat` utilise une optimisation de cache via les sticky sessions. Le client peut transmettre un en-tÃªte HTTP personnalisÃ© `X-Session-ID`.

Cela permet Ã  FastAPI de router toujours vers le mÃªme pod vLLM, assurant la rÃ©utilisation du **KV cache** de gÃ©nÃ©ration de tokens (mÃ©moire GPU).

* âš¡ï¸ Cela accÃ©lÃ¨re les requÃªtes multi-turn / itÃ©ratives.
* ğŸ§  Si non spÃ©cifiÃ©, une session alÃ©atoire est gÃ©nÃ©rÃ©e automatiquement cÃ´tÃ© FastAPI.
* ğŸ”„ Si vous utilisez un client custom, conservez le mÃªme `X-Session-ID` pour les appels suivants.

---

## ğŸ“Š Test de charge

Utilise `post.lua` pour gÃ©nÃ©rer dynamiquement des prompts variÃ©s.

âš ï¸ Limitation : `wrk` applique un timeout de 2 secondes sur chaque requÃªte.

```bash
./tests/run_load_test.sh
```

Valeurs paramÃ©trables :

```bash
URL="http://fastapi.localhost/api/chat"
DURATION="30s"
THREADS=10
CONNECTIONS=100
SCRIPT="./tests/post.lua"
```

---

## âœï¸ Exemple complet de requÃªte avec paramÃ¨tres

Lâ€™API supporte les principaux paramÃ¨tres de sampling acceptÃ©s par vLLM. Voici un exemple :

```bash
curl -X POST http://fastapi.localhost/api/chat \
     -H "Content-Type: application/json" \
     -H "X-Session-ID: session-42" \
     -d '{
           "messages": [
             {"role": "user", "content": "Can you write a short story about a robot who discovers music?"}
           ],
           "temperature": 0.8,
           "top_p": 0.95,
           "top_k": 50,
           "max_tokens": 200,
           "stop": ["</s>", "\n"],
           "presence_penalty": 0.2,
           "frequency_penalty": 0.2,
           "repetition_penalty": 1.1,
           "logprobs": null,
         }'
```

---

## ğŸ‘¤ Auteur

Erwan Lavenant â€“ 2025
