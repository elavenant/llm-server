# üß† LLM Server ‚Äì FastAPI + vLLM on Kubernetes

Ce projet expose une API REST (`/api/chat`) pour interagir avec un mod√®le de langage via [vLLM](https://github.com/vllm-project/vllm), avec support GPU, batching, et mise √† l‚Äô√©chelle Kubernetes.

## üöÄ Fonctionnalit√©s

- API REST via FastAPI
- Mod√®le LLM servi via vLLM
- Sticky session via header pour optimisation KV cache
- Configuration dynamique via Helm
- Logs formatt√©s JSON sur stdout pr√™ts pour collecte
- Monitoring Prometheus (/metrics expos√©s)
- Test de charge via `wrk` 

---

## üì¶ D√©ploiement

### 1. Pr√©requis

- Kubernetes (test√© avec `minikube`)
- Helm ‚â• 3.x
- GPU (ou `useGPU: false` pour CPU-only). Attention, vLLM tr√®s instable sur CPU, nombreuses erreurs relev√©es et non support de WSL2 + CPU 
- NGINX ingress controller ou Traefik configurable (non d√©ploy√©)

### 2. Build local des images

```bash
docker build -t <nom_de_repo>/fastapi:<tag>-f Dockerfile.fastapi .
docker build -t <nom_de_repo>/vllm-tinyllama:<tag> -f Dockerfile.vllm . # N√©cessite de placer un mod√®le support√© vLLM dans /models
```

### 3. D√©ploiement avec Helm

```bash
helm upgrade --install llm-app ./helm-chart \
  --namespace llm-app --create-namespace
  ```

  Exemple de values par d√©faut:

```yaml
namespace: llm-app

fastapi:
  name: fastapi
  image: docktet/fastapi:v1.1.9 # Image publique avec mod√®le pr√©charg√© 
  serviceAccount:
    name: fastapi
  containerPort: 3000
  replicaCount: 2
  modelDisplayName: "TinyLlama-1.1B" # Nom de mod√®le affich√© dans les r√©ponses
  resources:
    requests:
      cpu: "250m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

vllm:
  name: vllm
  image:
    repository: docktet/vllm-tinyllama # Image publique avec mod√®le pr√©charg√© 
    tag: latest
  port: 8000
  replicaCount: 1
  useGPU: true # si True, GPU utilis√© (n√©cessite support de Nvidia device plugin)
  resources:
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

ingress:
  enabled: true  
  type: standard  # "traefik" or "standard"
  className: nginx  # ingress class name (e.g., nginx, traefik)
  host: fastapi.localhost

```
### 4. Nettoyage

```bash
helm uninstall llm-app -n <namespace_name>
kubectl delete namespace <namespace_name>
```


## Endpoints expos√©s 

/api/chat	Endpoint principal pour interroger le mod√®le
/metrics	Export Prometheus
/ready	Readiness probe
/healthz	Liveness probe


## Test de charge

```bash
./tests/run_load_test.sh
```
Valeurs param√©trables:

URL="http://fastapi.localhost/api/chat"
DURATION="30s"
THREADS=10
CONNECTIONS=100
SCRIPT="./tests/post.lua"


