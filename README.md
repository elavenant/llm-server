# LLM Server ‚Äì FastAPI + vLLM on Kubernetes

Ce projet expose une API REST (`/api/chat`) pour interagir avec un mod√®le de langage via [vLLM](https://github.com/vllm-project/vllm), avec support GPU, batching, sticky session pour KV cache optimisation et mise √† l‚Äô√©chelle Kubernetes.

## üöÄ Fonctionnalit√©s

* API REST via FastAPI
* Mod√®le LLM servi via vLLM
* Sticky session via header pour optimisation KV cache
* Configuration dynamique via Helm
* Logs formatt√©s JSON sur stdout pr√™ts pour collecte
* Monitoring Prometheus (/metrics expos√©s)
* Test de charge via `wrk`

---

## üì¶ D√©ploiement

### 1. Pr√©requis

* Kubernetes (test√© avec `minikube`)
* Helm ‚â• 3.x
* GPU (ou `useGPU: false` pour CPU-only). ‚ö†Ô∏è vLLM tr√®s instable sur CPU, nombreuses erreurs relev√©es et non support de WSL2 + CPU
* NGINX ingress controller ou Traefik configurable (non d√©ploy√©)

### 2. Images Docker

Deux options sont possibles :

#### a. Utiliser les images pr√©-construites du d√©p√¥t public DockerHub (TinyLlama-1.1B)

```yaml
fastapi.image: docktet/fastapi:v1.1.9
vllm.image.repository: docktet/vllm-tinyllama # ~11 Gb
vllm.image.tag: latest
```

#### b. Construire les images localement (avec choix de mod√®le)

```bash
docker build -t <nom_de_repo>/<nom_image>:<tag> -f Dockerfile.fastapi .
docker build -t <nom_de_repo>/<nom_image>:<tag> -f Dockerfile.vllm . # N√©cessite de placer un mod√®le support√© vLLM dans /models
```

### 3. D√©ploiement avec Helm

```bash
helm upgrade --install llm-app ./helm-chart \
  --namespace llm-app --create-namespace
```

#### Exemple de values.yaml :

```yaml
namespace: llm-app

fastapi:
  name: fastapi
  image: docktet/fastapi:latest # Image publique avec mod√®le pr√©charg√© 
  serviceAccount:
    name: fastapi
  containerPort: 3000
  replicaCount: 2
  modelDisplayName: "TinyLlama-1.1B" # Nom de mod√®le affich√© dans les r√©ponses
  resources:
    requests:
      cpu: "250m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

vllm:
  name: vllm
  image:
    repository: docktet/vllm-tinyllama # Image publique avec mod√®le pr√©charg√© 
    tag: latest
  port: 8000
  replicaCount: 1
  useGPU: true # Si True, GPU utilis√© (n√©cessite support de Nvidia device plugin)
  resources: 
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "2"
      memory: "8Gi"

ingress:
  enabled: true # D√©ploiement ingress pour exposition si true
  type: standard # "traefik" ou "standard"
  className: nginx # Ingress class name (e.g., nginx, traefik)
  host: fastapi.localhost
```

### 4. Nettoyage

```bash
helm uninstall llm-app -n llm-app
kubectl delete namespace llm-app
```

---

## üåê Endpoints expos√©s

| Endpoint    | Description          |
| ----------- | -------------------- |
| `/api/chat` | Interroger le mod√®le |
| `/metrics`  | Export Prometheus    |
| `/ready`    | Readiness probe      |
| `/healthz`  | Liveness probe       |

---

## üßπ Architecture ‚Äì composants principaux

L‚Äôapplication est compos√©e de deux services principaux d√©ploy√©s dans Kubernetes¬†:

* **FastAPI Gateway**

  * Expose `/api/chat`, `/metrics`, `/healthz`, `/ready`
  * R√©partit les requ√™tes vers vLLM via `X-Session-ID`
  * Int√®gre un client Kubernetes pour d√©couverte dynamique des pods vLLM

* **vLLM Backend**

  * Expose les endpoints OpenAI compatibles (`/v1/chat/completions`, etc.)
  * Effectue la g√©n√©ration de texte avec batching et cache KV GPU
  * Peut √™tre configur√© avec ou sans GPU (‚ö†Ô∏è CPU instable)

üîÅ Ces services communiquent via le r√©seau interne du cluster (service Kubernetes).

---

## üîÅ Sticky sessions (optimisation cache)

L‚ÄôAPI `/api/chat` utilise une optimisation de cache via les sticky sessions. Le client peut transmettre un en-t√™te HTTP personnalis√© `X-Session-ID`.

Cela permet √† FastAPI de router toujours vers le m√™me pod vLLM, assurant la r√©utilisation du **KV cache** de g√©n√©ration de tokens (m√©moire GPU).

* ‚ö°Ô∏è Cela acc√©l√®re les requ√™tes multi-turn / it√©ratives.
* üß† Si non sp√©cifi√©, une session al√©atoire est g√©n√©r√©e automatiquement c√¥t√© FastAPI.
* üîÑ Si vous utilisez un client custom, conservez le m√™me `X-Session-ID` pour les appels suivants.

Test du sticky session: 

1. D√©ploiement du mock backend
```bash
kubectl run mock-vllm-backend --image=docktet/mock-vllm-backend:latest --namespace=llm-app --labels="app=llm-app-vllm"
```

2. Execution du script
```bash
./tests/test_sticky_routing.sh
```

---

## üìä Test de charge

Utilise `post.lua` pour g√©n√©rer dynamiquement des prompts vari√©s.

‚ö†Ô∏è Limitation : `wrk` applique un timeout de 2 secondes sur chaque requ√™te.

```bash
./tests/run_load_test.sh
```

Valeurs param√©trables :

```bash
- URL="http://fastapi.localhost/api/chat"
- DURATION="30s"
-  THREADS=10
- CONNECTIONS=100
- SCRIPT="./tests/post.lua"
```

Le test peut √©galement √™tre lanc√© manuellement avec la commande suivante (gestion du timeout):

```bash
wrk -t10 -c100 -d30s --timeout 5s -s tests/post.lua http://fastapi.localhost/api/chat
```

---

## ‚úçÔ∏è Exemple complet de requ√™te avec param√®tres

L‚ÄôAPI supporte les principaux param√®tres de sampling accept√©s par vLLM. Voici un exemple :

```bash
curl -X POST http://fastapi.localhost/api/chat \
     -H "Content-Type: application/json" \
     -H "X-Session-ID: session-42" \
     -d '{
           "messages": [
             {"role": "user", "content": "Can you write a short story about a robot who discovers music?"}
           ],
           "temperature": 0.8,
           "top_p": 0.95,
           "top_k": 50,
           "max_tokens": 200,
           "stop": ["</s>", "\n"],
           "presence_penalty": 0.2,
           "frequency_penalty": 0.2,
           "repetition_penalty": 1.1,
           "logprobs": null
         }'
```

---

## üë§ Auteur

Erwan Lavenant ‚Äì 2025
